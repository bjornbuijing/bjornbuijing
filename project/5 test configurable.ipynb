{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for changing models and parameters.\n",
    "Apologies for the lack of a proper poetry implementation. I wasn't sure the windows implementation (windows-curses) would throw a spanner into the works and I ran out of time to make a proper distributable package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import gin\n",
    "from DataUtils.Getdata import GetDataSets\n",
    "from DataUtils.Getdata import ModuleTest\n",
    "from Models.LineairModelCollection import CNN\n",
    "import Models.LineairModelCollection\n",
    "\n",
    "print(torch.version.cuda)\n",
    "gin.enter_interactive_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedConfigFileIncludesAndImports(filename='config.gin', imports=[], includes=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin.parse_config_file(\"config.gin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "train_dataloader,test_dataloader = GetDataSets(batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset\n",
    "type(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "X.shape, y.shape\n",
    "\n",
    "flat = nn.Flatten()\n",
    "z = flat(X)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = X[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdef4cb1790>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "globaLlr = 0.02\n",
    "globalEpochs = 10\n",
    "globalsteps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 15:16:41.799 | INFO     | Training.trainer:RunTrainer:71 - Logging to ..\\trained_models\\BRBSequentialLow\\/20220524-1516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRBSequentialLow(\n",
      "  (dense): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=392, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=392, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "..\\trained_models\\BRBSequentialLow\\\n",
      "Epoch : 0 - train loss =0.009295086888968945\n",
      "testloss :0.00901151867583394 -  test accuracy :0.8177083134651184\n",
      "Epoch : 1 - train loss =0.007300118097787102\n",
      "testloss :0.008110451822479567 -  test accuracy :0.8333333134651184\n",
      "Epoch : 2 - train loss =0.007012746451298396\n",
      "testloss :0.00671760148058335 -  test accuracy :0.8489583134651184\n",
      "Epoch : 3 - train loss =0.006931191532313824\n",
      "testloss :0.008875314456721147 -  test accuracy :0.8072916865348816\n",
      "Epoch : 4 - train loss =0.00683888526186347\n",
      "testloss :0.007389550718168418 -  test accuracy :0.84375\n",
      "Epoch : 5 - train loss =0.006832676541060209\n",
      "testloss :0.006737591543545325 -  test accuracy :0.8645833134651184\n",
      "Epoch : 6 - train loss =0.006542129131282369\n",
      "testloss :0.007398912062247594 -  test accuracy :0.859375\n",
      "Epoch : 7 - train loss =0.006649229661375284\n",
      "testloss :0.010060764538745085 -  test accuracy :0.8125\n",
      "Epoch : 8 - train loss =0.0066427247906724616\n",
      "testloss :0.0046178316697478294 -  test accuracy :0.8854166865348816\n",
      "Epoch : 9 - train loss =0.006439090956623356\n",
      "testloss :0.008020514777551094 -  test accuracy :0.8020833134651184\n"
     ]
    }
   ],
   "source": [
    "from Models.LineairModelCollection import BRBSequentialLow  \n",
    "\n",
    "model = BRBSequentialLow().to(device)\n",
    "print(model)\n",
    "\n",
    "from Training.trainer import RunTrainer\n",
    "import torch.optim as optim\n",
    "adamOpt = optim.Adam\n",
    "lossCross = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "RunTrainer(model=model, \n",
    "           train_dataloader=train_dataloader,\n",
    "           test_dataloader=test_dataloader,\n",
    "           learning_rate=globaLlr,\n",
    "           epochs=globalEpochs,\n",
    "           optimizer=adamOpt,\n",
    "           loss_fn=lossCross,\n",
    "           eval_steps=3,\n",
    "           device=device,\n",
    "           log_dir='..\\\\trained_models\\\\BRBSequentialLow\\\\'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 10:25:42.557 | INFO     | Training.trainer:RunTrainer:71 - Logging to ..\\trained_models\\BRBSequentialHigh\\20220519-1025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRBSequentialHigh(\n",
      "  (dense): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=784, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=784, out_features=784, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=784, out_features=784, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=784, out_features=784, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=784, out_features=784, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=784, out_features=784, bias=True)\n",
      "    (12): ReLU()\n",
      "    (13): Linear(in_features=784, out_features=392, bias=True)\n",
      "    (14): ReLU()\n",
      "    (15): Linear(in_features=392, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "..\\trained_models\\BRBSequentialHigh\n",
      "Epoch : 0 - train loss =0.035985493286450704\n",
      "testloss :0.035940914104382195 -  test accuracy :0.1145833358168602\n",
      "Epoch : 1 - train loss =0.03582356862624486\n",
      "testloss :0.03502980122963587 -  test accuracy :0.3958333432674408\n",
      "Epoch : 2 - train loss =0.023293038230141005\n",
      "testloss :0.017011804195741814 -  test accuracy :0.5572916865348816\n",
      "Epoch : 3 - train loss =0.015031729731957117\n",
      "testloss :0.013945518371959528 -  test accuracy :0.6458333730697632\n",
      "Epoch : 4 - train loss =0.011818116426467896\n",
      "testloss :0.012417211197316647 -  test accuracy :0.7083333730697632\n",
      "Epoch : 5 - train loss =0.009506270176668961\n",
      "testloss :0.009130197732398907 -  test accuracy :0.7708333730697632\n",
      "Epoch : 6 - train loss =0.008254477518300215\n",
      "testloss :0.008595741353929043 -  test accuracy :0.7916666865348816\n",
      "Epoch : 7 - train loss =0.0074361462046702706\n",
      "testloss :0.00809641694650054 -  test accuracy :0.8177083730697632\n",
      "Epoch : 8 - train loss =0.006785762889186541\n",
      "testloss :0.007042632283022006 -  test accuracy :0.8229166865348816\n",
      "Epoch : 9 - train loss =0.006331814647714297\n",
      "testloss :0.00732129563887914 -  test accuracy :0.8645833730697632\n",
      "Epoch : 10 - train loss =0.005877710680911938\n",
      "testloss :0.008083641373862823 -  test accuracy :0.8541666865348816\n",
      "Epoch : 11 - train loss =0.005507395367076\n",
      "testloss :0.007873665075749159 -  test accuracy :0.796875\n",
      "Epoch : 12 - train loss =0.0051888091170539455\n",
      "testloss :0.008272148668766022 -  test accuracy :0.84375\n",
      "Epoch : 13 - train loss =0.004959037677322825\n",
      "testloss :0.003969358513131738 -  test accuracy :0.9114583730697632\n",
      "Epoch : 14 - train loss =0.004759422144417961\n",
      "testloss :0.005659180227667093 -  test accuracy :0.875\n",
      "Epoch : 15 - train loss =0.004548953173557917\n",
      "testloss :0.006502519982556502 -  test accuracy :0.8177083730697632\n",
      "Epoch : 16 - train loss =0.00436840941136082\n",
      "testloss :0.00565699200766782 -  test accuracy :0.890625\n",
      "Epoch : 17 - train loss =0.004393016521756847\n",
      "testloss :0.005586366945256789 -  test accuracy :0.8697916865348816\n",
      "Epoch : 18 - train loss =0.004071453672399123\n",
      "testloss :0.007285131607204676 -  test accuracy :0.8489583730697632\n",
      "Epoch : 19 - train loss =0.003917372657606999\n",
      "testloss :0.006574288010597229 -  test accuracy :0.8489583730697632\n",
      "Epoch : 20 - train loss =0.0037704838819801806\n",
      "testloss :0.006932722094158332 -  test accuracy :0.859375\n",
      "Epoch : 21 - train loss =0.003720650681356589\n",
      "testloss :0.008824030713488659 -  test accuracy :0.8489583730697632\n",
      "Epoch : 22 - train loss =0.0035739867728824416\n",
      "testloss :0.0068230899050831795 -  test accuracy :0.8645833730697632\n",
      "Epoch : 23 - train loss =0.0034958789221321545\n",
      "testloss :0.003552319404358665 -  test accuracy :0.9166666865348816\n",
      "Epoch : 24 - train loss =0.003342332031143208\n",
      "testloss :0.007359933108091354 -  test accuracy :0.8385416865348816\n",
      "Epoch : 25 - train loss =0.003277229154140999\n",
      "testloss :0.006162651814520359 -  test accuracy :0.8645833730697632\n",
      "Epoch : 26 - train loss =0.003178810812222461\n",
      "testloss :0.00581051471332709 -  test accuracy :0.8541666865348816\n",
      "Epoch : 27 - train loss =0.003262730555670957\n",
      "testloss :0.005687208070109288 -  test accuracy :0.890625\n",
      "Epoch : 28 - train loss =0.0030631896814331413\n",
      "testloss :0.005830793796728055 -  test accuracy :0.8854166865348816\n",
      "Epoch : 29 - train loss =0.00292602412380899\n",
      "testloss :0.005252577054003875 -  test accuracy :0.8802083730697632\n",
      "Epoch : 30 - train loss =0.0029143996932854254\n",
      "testloss :0.004385174096872409 -  test accuracy :0.90625\n",
      "Epoch : 31 - train loss =0.0027945448046860595\n",
      "testloss :0.008462666533887386 -  test accuracy :0.84375\n",
      "Epoch : 32 - train loss =0.0027639993473887446\n",
      "testloss :0.00755547018100818 -  test accuracy :0.8802083730697632\n",
      "Epoch : 33 - train loss =0.0026443756488151848\n",
      "testloss :0.010011420585215092 -  test accuracy :0.828125\n",
      "Epoch : 34 - train loss =0.0025636276859169207\n",
      "testloss :0.004668982660708328 -  test accuracy :0.921875\n",
      "Epoch : 35 - train loss =0.0024768255352663497\n",
      "testloss :0.0058480920270085335 -  test accuracy :0.8854166865348816\n",
      "Epoch : 36 - train loss =0.0024251557001533607\n",
      "testloss :0.0046309169847518206 -  test accuracy :0.9114583730697632\n",
      "Epoch : 37 - train loss =0.0024189563953628144\n",
      "testloss :0.009892246996363005 -  test accuracy :0.84375\n",
      "Epoch : 38 - train loss =0.0023637365651937824\n",
      "testloss :0.006175370421260595 -  test accuracy :0.8697916865348816\n",
      "Epoch : 39 - train loss =0.0022587838312300543\n",
      "testloss :0.006608462582031886 -  test accuracy :0.8802083730697632\n",
      "Epoch : 40 - train loss =0.0024963795649819076\n",
      "testloss :0.006506124356140693 -  test accuracy :0.8802083730697632\n",
      "Epoch : 41 - train loss =0.0021455433938652275\n",
      "testloss :0.0031055993555734553 -  test accuracy :0.9270833730697632\n",
      "Epoch : 42 - train loss =0.0020765106371603906\n",
      "testloss :0.006714535101006429 -  test accuracy :0.90625\n",
      "Epoch : 43 - train loss =0.002023437147773802\n",
      "testloss :0.0053648087196052074 -  test accuracy :0.890625\n",
      "Epoch : 44 - train loss =0.0019605802939273416\n",
      "testloss :0.005737108100826542 -  test accuracy :0.8802083730697632\n",
      "Epoch : 45 - train loss =0.0018969010430853813\n",
      "testloss :0.0068079450478156405 -  test accuracy :0.8958333730697632\n",
      "Epoch : 46 - train loss =0.001876491042630126\n",
      "testloss :0.0042566491368537145 -  test accuracy :0.9166666865348816\n",
      "Epoch : 47 - train loss =0.0018645683045033366\n",
      "testloss :0.010321242734789848 -  test accuracy :0.8333333730697632\n",
      "Epoch : 48 - train loss =0.0018467540609650313\n",
      "testloss :0.00517287221737206 -  test accuracy :0.8958333730697632\n",
      "Epoch : 49 - train loss =0.001722880039162313\n",
      "testloss :0.006331855586419503 -  test accuracy :0.859375\n"
     ]
    }
   ],
   "source": [
    "from Models.LineairModelCollection import BRBSequentialHigh  \n",
    "\n",
    "model = BRBSequentialHigh().to(device)\n",
    "model.cuda()\n",
    "print(model)\n",
    "\n",
    "from Training.trainer import RunTrainer\n",
    "import torch.optim as optim\n",
    "adamOpt = optim.Adam\n",
    "lossCross = torch.nn.CrossEntropyLoss()\n",
    "lossCross.cuda()\n",
    "\n",
    "RunTrainer(model=model, \n",
    "           train_dataloader=train_dataloader,\n",
    "           test_dataloader=test_dataloader,\n",
    "           learning_rate=globaLlr,\n",
    "           epochs=globalEpochs,\n",
    "           optimizer=adamOpt,\n",
    "           loss_fn=lossCross,\n",
    "           eval_steps=3,\n",
    "           device=device,\n",
    "           log_dir='..\\\\trained_models\\\\BRBSequentialHigh\\\\'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/mladmin/code/Project/bjornbuijing/project/5 test configurable.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d616368696e654c6561726e696e67564d227d/home/mladmin/code/Project/bjornbuijing/project/5%20test%20configurable.ipynb#ch0000017vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mModels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mLineairModelCollection\u001b[39;00m \u001b[39mimport\u001b[39;00m BRBSequentialVariable\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d616368696e654c6561726e696e67564d227d/home/mladmin/code/Project/bjornbuijing/project/5%20test%20configurable.ipynb#ch0000017vscode-remote?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m BRBSequentialVariable()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d616368696e654c6561726e696e67564d227d/home/mladmin/code/Project/bjornbuijing/project/5%20test%20configurable.ipynb#ch0000017vscode-remote?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d616368696e654c6561726e696e67564d227d/home/mladmin/code/Project/bjornbuijing/project/5%20test%20configurable.ipynb#ch0000017vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d616368696e654c6561726e696e67564d227d/home/mladmin/code/Project/bjornbuijing/project/5%20test%20configurable.ipynb#ch0000017vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mTraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m RunTrainer\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=670'>671</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=671'>672</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=672'>673</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=673'>674</a>\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=685'>686</a>\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=686'>687</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=687'>688</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=596'>597</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=597'>598</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=598'>599</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=599'>600</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=600'>601</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=601'>602</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=602'>603</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=670'>671</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=671'>672</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=672'>673</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=673'>674</a>\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=685'>686</a>\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=686'>687</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py?line=687'>688</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py?line=211'>212</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py?line=212'>213</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py?line=213'>214</a>\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py?line=214'>215</a>\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py?line=215'>216</a>\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py?line=216'>217</a>\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py?line=217'>218</a>\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py?line=218'>219</a>\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/mladmin/.cache/pypoetry/virtualenvs/deep-learning-E14Cnx23-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py?line=219'>220</a>\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "from Models.LineairModelCollection import BRBSequentialVariable\n",
    "\n",
    "model = BRBSequentialVariable().to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "from Training.trainer import RunTrainer\n",
    "import torch.optim as optim\n",
    "adamOpt = optim.Adam\n",
    "lossCross = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "RunTrainer(model=model, \n",
    "           train_dataloader=train_dataloader,\n",
    "           test_dataloader=test_dataloader,\n",
    "           learning_rate=globaLlr,\n",
    "           epochs=globalEpochs,\n",
    "           optimizer=adamOpt,\n",
    "           loss_fn=lossCross,\n",
    "           eval_steps=3,\n",
    "           device=device,\n",
    "           log_dir='..\\\\trained_models\\\\BRBSequentialVariable\\\\'\n",
    "           )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b8f312320cd240106b9ea4d318428341e8727b3c7d5fc1f73cfe4a3d9868ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deep-learning-E14Cnx23-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
